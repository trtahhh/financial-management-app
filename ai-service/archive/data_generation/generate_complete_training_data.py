#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Improved training data generator for 14 categories.

Goals of refactor:
1. Reduce duplication & overfitting (remove brute-force upper/title variants)
2. Increase linguistic coverage via rule‚Äëbased template expansion
3. Provide diacritic + case normalized variants (Vietnamese no-accent forms)
4. Ensure uniqueness & balanced sampling
5. Allow configurable sample counts & train/test split

Usage:
  python generate_complete_training_data.py --samples 180 --split 0.85 --seed 42

Output:
  vietnamese_transactions_14categories.json  (full shuffled dataset)
  vietnamese_transactions_14categories_train.json
  vietnamese_transactions_14categories_test.json
"""

import json
import random
import argparse
from typing import Dict, List, Set, Tuple

# --- Core lexical resources -------------------------------------------------

# Base synonyms per category (concise; semantic coverage > raw count)
CATEGORY_SYNONYMS: Dict[str, List[str]] = {
    "L∆∞∆°ng": ["l∆∞∆°ng", "m·ª©c l∆∞∆°ng", "salary", "thu nh·∫≠p", "ti·ªÅn c√¥ng", "payroll", "wage"],
    "Thu nh·∫≠p kh√°c": ["thu nh·∫≠p th√™m", "extra income", "bonus", "hoa h·ªìng", "incentive", "tip", "freelance", "th∆∞·ªüng"],
    "ƒê·∫ßu t∆∞": ["ƒë·∫ßu t∆∞", "ch·ª©ng kho√°n", "c·ªï phi·∫øu", "crypto", "bitcoin", "qu·ªπ", "l√£i ti·∫øt ki·ªám", "v√†ng", "bond", "t√≠n phi·∫øu"],
    "Kinh doanh": ["kinh doanh", "doanh thu", "b√°n h√†ng", "revenue", "profit", "thu cho thu√™", "passive income", "l·ª£i nhu·∫≠n"],
    "ƒÇn u·ªëng": ["ph·ªü", "b√∫n", "c∆°m", "b√°nh m√¨", "ƒÉn s√°ng", "ƒÉn tr∆∞a", "ƒÉn t·ªëi", "ƒë·ªì ƒÉn", "nh√† h√†ng", "qu√°n ƒÉn", "cafe", "tr√† s·ªØa", "fast food", "buffet", "delivery", "g·ªçi ƒë·ªì ƒÉn"],
    "Giao th√¥ng": ["xƒÉng xe", "grab", "taxi", "be", "gojek", "v√© m√°y bay", "v√© xe bus", "b·∫£o d∆∞·ª°ng xe", "parking", "s·ª≠a xe", "r·ª≠a xe", "thay nh·ªõt"],
    "Gi·∫£i tr√≠": ["xem phim", "cgv", "karaoke", "game", "netflix", "spotify", "concert", "party", "gi·∫£i tr√≠", "bi-a", "bowling"],
    "S·ª©c kh·ªèe": ["kh√°m b·ªánh", "mua thu·ªëc", "b·∫£o hi·ªÉm", "gym", "yoga", "x√©t nghi·ªám", "dental", "massage", "spa", "nha khoa", "b·ªánh vi·ªán"],
    "Gi√°o d·ª•c": ["h·ªçc ph√≠", "mua s√°ch", "kh√≥a h·ªçc", "training", "ch·ª©ng ch·ªâ", "gia s∆∞", "stationery", "h·ªçc t·∫≠p", "ti·∫øng anh", "ielts"],
    "Mua s·∫Øm": ["mua √°o", "mua qu·∫ßn", "gi√†y", "shopping", "ƒëi·ªán tho·∫°i", "laptop", "ƒë·ªì ƒëi·ªán t·ª≠", "th·ªùi trang", "online", "shopee", "lazada"],
    "Ti·ªán √≠ch": ["ti·ªÅn ƒëi·ªán", "ti·ªÅn n∆∞·ªõc", "internet", "wifi", "thu√™ nh√†", "rent", "gas", "ph√≠ d·ªãch v·ª•", "ph√≠ qu·∫£n l√Ω"],
    "Vay n·ª£": ["tr·∫£ n·ª£", "vay ng√¢n h√†ng", "credit", "loan", "tr·∫£ g√≥p", "interest", "mortgage", "debt", "th·∫ª t√≠n d·ª•ng"],
    "Qu√† t·∫∑ng": ["qu√† sinh nh·∫≠t", "t·ª´ thi·ªán", "donation", "l√¨ x√¨", "gift", "charity", "·ªßng h·ªô", "t·∫∑ng qu√†", "m·ª´ng c∆∞·ªõi"],
    "Kh√°c": ["chi ph√≠ kh√°c", "service fee", "ph√≠ giao d·ªãch", "withdrawal fee", "subscription", "membership", "admin fee", "misc"]
}

# Action / context verbs to enrich descriptions
VERBS = ["nh·∫≠n", "tr·∫£", "mua", "ƒë√≥ng", "thanh to√°n", "chi", "ƒë·∫ßu t∆∞", "n·ªôp", "s·ª≠ d·ª•ng", "gia h·∫°n"]
TIME_MODIFIERS = ["th√°ng n√†y", "th√°ng tr∆∞·ªõc", "h√¥m nay", "tu·∫ßn n√†y", "qu√Ω n√†y"]
MONTHS = [str(m) for m in range(1, 13)]

# Mapping category metadata
CATEGORY_ID = {
    "L∆∞∆°ng": 1, "Thu nh·∫≠p kh√°c": 2, "ƒê·∫ßu t∆∞": 3, "Kinh doanh": 4,
    "ƒÇn u·ªëng": 5, "Giao th√¥ng": 6, "Gi·∫£i tr√≠": 7, "S·ª©c kh·ªèe": 8,
    "Gi√°o d·ª•c": 9, "Mua s·∫Øm": 10, "Ti·ªán √≠ch": 11, "Vay n·ª£": 12,
    "Qu√† t·∫∑ng": 13, "Kh√°c": 14
}

TYPE_MAP = {k: ("income" if v <= 4 else "expense") for k, v in CATEGORY_ID.items()}

# --- Normalization utilities -------------------------------------------------
_VIETNAMESE_DIACRITIC_MAP = {
    # Simple mapping; not exhaustive but covers common characters in synonyms
    "√†":"a","√°":"a","·∫£":"a","√£":"a","·∫°":"a","ƒÉ":"a","·∫±":"a","·∫Ø":"a","·∫≥":"a","·∫µ":"a","·∫∑":"a","√¢":"a","·∫ß":"a","·∫•":"a","·∫©":"a","·∫´":"a","·∫≠":"a",
    "√®":"e","√©":"e","·∫ª":"e","·∫Ω":"e","·∫π":"e","√™":"e","·ªÅ":"e","·∫ø":"e","·ªÉ":"e","·ªÖ":"e","·ªá":"e",
    "√¨":"i","√≠":"i","·ªâ":"i","ƒ©":"i","·ªã":"i",
    "√≤":"o","√≥":"o","·ªè":"o","√µ":"o","·ªç":"o","√¥":"o","·ªì":"o","·ªë":"o","·ªï":"o","·ªó":"o","·ªô":"o","∆°":"o","·ªù":"o","·ªõ":"o","·ªü":"o","·ª°":"o","·ª£":"o",
    "√π":"u","√∫":"u","·ªß":"u","≈©":"u","·ª•":"u","∆∞":"u","·ª´":"u","·ª©":"u","·ª≠":"u","·ªØ":"u","·ª±":"u",
    "·ª≥":"y","√Ω":"y","·ª∑":"y","·ªπ":"y","·ªµ":"y",
    "ƒë":"d"
}

def strip_diacritics(text: str) -> str:
    return "".join(_VIETNAMESE_DIACRITIC_MAP.get(ch, ch) for ch in text.lower())

def unique(seq: List[str]) -> List[str]:
    seen: Set[str] = set()
    out = []
    for item in seq:
        key = item.strip()
        if key not in seen:
            seen.add(key)
            out.append(item)
    return out

# --- Variation generation ----------------------------------------------------

def generate_variations(base: str) -> List[str]:
    """Generate linguistic variations without naive upper/title duplication."""
    variants = {base.strip()}
    # Add diacritic-free version (for models trained on accent-insensitive corpora)
    variants.add(strip_diacritics(base))
    # Simple punctuation removal variant
    variants.add(base.replace("-", " ").replace(",", " "))
    return list(variants)

def build_descriptions(category: str, synonyms: List[str], samples_target: int, rng: random.Random) -> List[str]:
    pool: List[str] = []
    # Template expansions
    for syn in synonyms:
        syn_variants = generate_variations(syn)
        for var in syn_variants:
            # Base forms
            pool.append(var)
            # Verb + object
            for verb in rng.sample(VERBS, k=min(3, len(VERBS))):
                pool.append(f"{verb} {var}")
            # Time modifiers
            for tm in rng.sample(TIME_MODIFIERS, k=2):
                pool.append(f"{var} {tm}")
            # Month-specific (only for some financial contexts)
            if category in ("L∆∞∆°ng", "Thu nh·∫≠p kh√°c", "ƒê·∫ßu t∆∞", "Kinh doanh", "Ti·ªán √≠ch"):
                month = rng.choice(MONTHS)
                pool.append(f"{var} th√°ng {month}")
    # Deduplicate
    pool = unique(pool)
    # If pool smaller than requested, allow slight recombination
    if len(pool) < samples_target:
        extra_needed = samples_target - len(pool)
        for _ in range(extra_needed):
            base_choice = rng.choice(synonyms)
            verb = rng.choice(VERBS)
            tm = rng.choice(TIME_MODIFIERS)
            pool.append(f"{verb} {base_choice} {tm}")
        pool = unique(pool)
    # Sample down to target
    rng.shuffle(pool)
    return pool[:samples_target]

# --- Dataset assembly --------------------------------------------------------

def generate_training_dataset(samples_per_category: int = 180, seed: int = 42) -> List[Dict]:
    rng = random.Random(seed)
    dataset: List[Dict] = []
    for category, syns in CATEGORY_SYNONYMS.items():
        descriptions = build_descriptions(category, syns, samples_per_category, rng)
        for desc in descriptions:
            dataset.append({
                "description": desc,
                "category": category,
                "category_id": CATEGORY_ID[category],
                "type": TYPE_MAP[category]
            })
    rng.shuffle(dataset)
    return dataset

def train_test_split(dataset: List[Dict], split_ratio: float, seed: int) -> Tuple[List[Dict], List[Dict]]:
    rng = random.Random(seed)
    data = list(dataset)
    rng.shuffle(data)
    split_index = int(len(data) * split_ratio)
    return data[:split_index], data[split_index:]

def main():
    parser = argparse.ArgumentParser(description="Generate balanced, normalized training data")
    parser.add_argument("--samples", type=int, default=180, help="Samples per category (default 180)")
    parser.add_argument("--split", type=float, default=0.8, help="Train split ratio (default 0.8)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    print("=" * 78)
    print("GENERATING TRAINING DATA (Deduplicated / Template-Based)")
    print("=" * 78)
    print(f"-> Samples/category: {args.samples}\n-> Train split: {args.split}\n-> Seed: {args.seed}")

    dataset = generate_training_dataset(samples_per_category=args.samples, seed=args.seed)
    train, test = train_test_split(dataset, args.split, args.seed)

    print(f"\n‚úÖ Total samples: {len(dataset):,}")
    print(f"   ‚Ä¢ Train: {len(train):,} | Test: {len(test):,}")

    # Category distribution
    print("\nüìà Distribution:")
    dist = {}
    for row in dataset:
        dist[row['category']] = dist.get(row['category'], 0) + 1
    for cat in sorted(dist.keys(), key=lambda c: CATEGORY_ID[c]):
        print(f"   {cat:<15}: {dist[cat]:4d}")

    # Persist
    with open("vietnamese_transactions_14categories.json", "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    with open("vietnamese_transactions_14categories_train.json", "w", encoding="utf-8") as f:
        json.dump(train, f, ensure_ascii=False, indent=2)
    with open("vietnamese_transactions_14categories_test.json", "w", encoding="utf-8") as f:
        json.dump(test, f, ensure_ascii=False, indent=2)

    print("\nüíæ Saved:")
    print("   - vietnamese_transactions_14categories.json")
    print("   - vietnamese_transactions_14categories_train.json")
    print("   - vietnamese_transactions_14categories_test.json")

    print("\nüìã Sample (first 8):")
    for i, sample in enumerate(dataset[:8], 1):
        print(f"   {i}. {sample['description']} -> {sample['category']} ({sample['type']})")

    print("\n‚ú® Done. Consider further augmentation with contextual numeric amounts or POS tagging.")
    print("=" * 78)

if __name__ == "__main__":
    main()
